{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get the $\\nabla^2$operator in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tensorflow, the default 2nd derivative operator will contract over the hessian matrix in an undesirable way.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow  as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs(nwalkers, n_particles, dimension):\n",
    "    x = tf.random.uniform(shape=[nwalkers, n_particles, dimension])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates walkers in the same format as metropolis walkers, with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = generate_inputs(4,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below creates a scalar value: f(x, y) = $\\alpha x^2 + \\beta y^2 + \\gamma x y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavefunction(inputs, alpha=0.1, beta=0.2, gamma=0.5):\n",
    "    # return x^2 + y^2 + xy, with constants:\n",
    "    ret = alpha * inputs[:,:,0]**2\n",
    "    ret += beta * inputs[:,:,1]**2\n",
    "    ret += gamma * inputs[:,:,0]*inputs[:,:,1]\n",
    "    \n",
    "    return tf.squeeze(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = wavefunction(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the forward pass, telling tensorflow to watch the inputs since that's what we want to differentiate with respect to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = generate_inputs(4,1,2)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as outer_tape:\n",
    "    outer_tape.watch(inputs)\n",
    "    with tf.GradientTape(persistent=True) as inner_tape:\n",
    "        inner_tape.watch(inputs)\n",
    "        outputs = wavefunction(inputs)\n",
    "    # Compute the first derivative with respect to the inputs\n",
    "    dw_dx = inner_tape.gradient(outputs, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have to compute the first derivative, above, within the block of the outer_tape to compute a second derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should have the same shape as the inputs, and the values are analytically computable (and thus checkable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dw_dx.shape == inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic_derivative(inputs, alpha=0.1, beta=0.2, gamma=0.5):\n",
    "    _x = inputs[:,:,0]\n",
    "    _y = inputs[:,:,1]\n",
    "    \n",
    "    x = 2*alpha*_x + gamma*_y\n",
    "    y = 2*beta*_y  + gamma*_x\n",
    "    \n",
    "    return tf.stack([x,y], axis=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 2), dtype=float32, numpy=\n",
       "array([[[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw_dx - analytic_derivative(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge, then, is the 2nd derivative.  We know what it *should* be if we're computing $\\nabla^2$, but this is not what tensorflow computes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nabla_squared(inputs, alpha=0.1, beta=0.2, gamma=0.5):\n",
    "    _x = tf.constant(2 * alpha, shape=inputs[:,:,0].shape)\n",
    "    _y = tf.constant(2*beta, shape=inputs[:,:,1].shape)\n",
    "    \n",
    "    return tf.stack([_x, _y], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 2), dtype=float32, numpy=\n",
       "array([[[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nabla_squared(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2w_dx2 = outer_tape.gradient(dw_dx, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.7 0.9]]\n",
      "\n",
      " [[0.7 0.9]]\n",
      "\n",
      " [[0.7 0.9]]\n",
      "\n",
      " [[0.7 0.9]]], shape=(4, 1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(d2w_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely, each value here is off by $\\gamma$ - tensorflow is contracting the hessian to compute this second derivative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[[[0.2 0.5]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]]\n",
      "\n",
      "\n",
      "   [[[0.5 0.4]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [[[[[0.  0. ]]\n",
      "\n",
      "    [[0.2 0.5]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]]\n",
      "\n",
      "\n",
      "   [[[0.  0. ]]\n",
      "\n",
      "    [[0.5 0.4]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [[[[[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.2 0.5]]\n",
      "\n",
      "    [[0.  0. ]]]\n",
      "\n",
      "\n",
      "   [[[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.5 0.4]]\n",
      "\n",
      "    [[0.  0. ]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [[[[[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.2 0.5]]]\n",
      "\n",
      "\n",
      "   [[[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.5 0.4]]]]]], shape=(4, 1, 2, 4, 1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "d2wdx2 = outer_tape.jacobian(dw_dx, inputs)\n",
    "print(d2wdx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jacobian comes out with a dimension that is much too big.  We can contract it with \"einsum\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 2), dtype=float32, numpy=\n",
       "array([[[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.einsum(\"wpdwpd->wpd\",d2wdx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, that is the correct value for $\\nabla^2$ of this function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an equivalent implementation in our case where the output (per \"batch\") depends only on that input \"batch\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x14ad52ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 2), dtype=float32, numpy=\n",
       "array([[[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.einsum(\"wpdpd->wpd\",outer_tape.batch_jacobian(dw_dx, inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
