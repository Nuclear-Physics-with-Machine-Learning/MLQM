{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get the $\\nabla^2$operator in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tensorflow, the default 2nd derivative operator will contract over the hessian matrix in an undesirable way.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow  as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs(nwalkers, n_particles, dimension):\n",
    "    x = tf.random.uniform(shape=[nwalkers, n_particles, dimension])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates walkers in the same format as metropolis walkers, with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = generate_inputs(4,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below creates a scalar value: f(x, y) = $\\alpha x^2 + \\beta y^2 + \\gamma x y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavefunction(inputs, alpha=0.1, beta=0.2, gamma=0.5):\n",
    "    # return x^2 + y^2 + xy, with constants:\n",
    "    ret = alpha * inputs[:,:,0]**2\n",
    "    ret += beta * inputs[:,:,1]**2\n",
    "    ret += gamma * inputs[:,:,0]*inputs[:,:,1]\n",
    "    \n",
    "    return tf.squeeze(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = wavefunction(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the forward pass, telling tensorflow to watch the inputs since that's what we want to differentiate with respect to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = generate_inputs(4,1,2)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as outer_tape:\n",
    "    outer_tape.watch(inputs)\n",
    "    with tf.GradientTape(persistent=True) as inner_tape:\n",
    "        inner_tape.watch(inputs)\n",
    "        outputs = wavefunction(inputs)\n",
    "    # Compute the first derivative with respect to the inputs\n",
    "    dw_dx = inner_tape.gradient(outputs, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have to compute the first derivative, above, within the block of the outer_tape to compute a second derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should have the same shape as the inputs, and the values are analytically computable (and thus checkable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dw_dx.shape == inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic_derivative(inputs, alpha=0.1, beta=0.2, gamma=0.5):\n",
    "    _x = inputs[:,:,0]\n",
    "    _y = inputs[:,:,1]\n",
    "    \n",
    "    x = 2*alpha*_x + gamma*_y\n",
    "    y = 2*beta*_y  + gamma*_x\n",
    "    \n",
    "    return tf.stack([x,y], axis=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 2), dtype=float32, numpy=\n",
       "array([[[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw_dx - analytic_derivative(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge, then, is the 2nd derivative.  We know what it *should* be if we're computing $\\nabla^2$, but this is not what tensorflow computes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nabla_squared(inputs, alpha=0.1, beta=0.2, gamma=0.5):\n",
    "    _x = tf.constant(2 * alpha, shape=inputs[:,:,0].shape)\n",
    "    _y = tf.constant(2*beta, shape=inputs[:,:,1].shape)\n",
    "    \n",
    "    return tf.stack([_x, _y], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 2), dtype=float32, numpy=\n",
       "array([[[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]]], dtype=float32)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nabla_squared(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2w_dx2 = outer_tape.gradient(dw_dx, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.7 0.9]]\n",
      "\n",
      " [[0.7 0.9]]\n",
      "\n",
      " [[0.7 0.9]]\n",
      "\n",
      " [[0.7 0.9]]], shape=(4, 1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(d2w_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely, each value here is off by $\\gamma$ - tensorflow is contracting the hessian to compute this second derivative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_tape.gradient(dw_dx, inputs[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 2), dtype=float32, numpy=\n",
       "array([[[0.7, 0.9]],\n",
       "\n",
       "       [[0.7, 0.9]],\n",
       "\n",
       "       [[0.7, 0.9]],\n",
       "\n",
       "       [[0.7, 0.9]]], dtype=float32)>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_tape.gradient(dw_dx, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 2), dtype=float32, numpy=\n",
       "array([[[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw_dx - analytic_derivative(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[[[0.2 0.5]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]]\n",
      "\n",
      "\n",
      "   [[[0.5 0.4]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [[[[[0.  0. ]]\n",
      "\n",
      "    [[0.2 0.5]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]]\n",
      "\n",
      "\n",
      "   [[[0.  0. ]]\n",
      "\n",
      "    [[0.5 0.4]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [[[[[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.2 0.5]]\n",
      "\n",
      "    [[0.  0. ]]]\n",
      "\n",
      "\n",
      "   [[[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.5 0.4]]\n",
      "\n",
      "    [[0.  0. ]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [[[[[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.2 0.5]]]\n",
      "\n",
      "\n",
      "   [[[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.  0. ]]\n",
      "\n",
      "    [[0.5 0.4]]]]]], shape=(4, 1, 2, 4, 1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "d2wdx2 = outer_tape.jacobian(dw_dx, inputs)\n",
    "print(d2wdx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jacobian comes out with a dimension that is much too big.  We can contract it with \"einsum\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 2), dtype=float32, numpy=\n",
       "array([[[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]]], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.einsum(\"wpdwpd->wpd\",d2wdx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, that is the correct value for $\\nabla^2$ of this function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does this take to run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 ms ± 6.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit d2wdx2 = tf.einsum(\"wpdwpd->wpd\", outer_tape.jacobian(dw_dx, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an equivalent implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 2), dtype=float32, numpy=\n",
       "array([[[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]],\n",
       "\n",
       "       [[0.2, 0.4]]], dtype=float32)>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.einsum(\"wpdpd->wpd\",outer_tape.batch_jacobian(dw_dx, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does this take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339 ms ± 5.93 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tf.einsum(\"wpdpd->wpd\",outer_tape.batch_jacobian(dw_dx, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also time the incorrect gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31 ms ± 12.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit outer_tape.gradient(dw_dx, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the CORRECT gradient is a good bit slower.  But, speed means  nothing if it's incorrect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
