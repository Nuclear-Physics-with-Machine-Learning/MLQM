{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydrogen Atom\n",
    "\n",
    "In the previous notebook, we solved the harmonic oscillator for 1 dimension using artifical neural networks.  In this notebook, we'll go a step further and solve a more complicated system (Hydrogen) in more dimensions (3 of course).  We will use more sophisticated sampling of the wavefunction and compute the proper $\\nabla^2$ operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "\n",
    "DEFAULT_TENSOR_TYPE = tf.float32\n",
    "N_WALKERS = 200\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Wavefunction\n",
    "\n",
    "For the hydrogen atom, we'll use a simple surrogate wavefunction.\n",
    "\n",
    "NOTE that the wavefunction is a surrogate for $log \\psi$, so the multiplicative initial boundary condition is a sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class NeuralWavefunction(tf.keras.models.Model):\n",
    "    \"\"\"Create a neural network eave function in N dimensions\n",
    "\n",
    "    Boundary condition, if not supplied, is gaussian in every dimension\n",
    "\n",
    "    Extends:\n",
    "        tf.keras.models.Model\n",
    "    \"\"\"\n",
    "    def __init__(self, ndim : int):\n",
    "        tf.keras.models.Model.__init__(self)\n",
    "\n",
    "        self.ndim = ndim\n",
    "\n",
    "\n",
    "        self.layer1 = tf.keras.layers.Dense(64, use_bias=False)\n",
    "        self.layer2 = tf.keras.layers.Dense(64, use_bias=False)\n",
    "        self.layer3 = tf.keras.layers.Dense(1, use_bias=False)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        # \n",
    "        # shape is [nwalkers, dim]\n",
    "        \n",
    "        x = self.layer1(inputs)\n",
    "        x = tf.keras.activations.tanh(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = tf.keras.activations.tanh(x)\n",
    "        \n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = tf.keras.activations.tanh(x)\n",
    "\n",
    "\n",
    "        # Compute the initial boundary condition, which the network will slowly overcome\n",
    "        # This is important because it keeps the walkers bounded, otherwise they will not converge to a bound state.\n",
    "        boundary_condition = -.1 * tf.reduce_sum(inputs**2, axis=(1))\n",
    "        boundary_condition = tf.reshape(boundary_condition, [-1,1])\n",
    "\n",
    "    \n",
    "        return x + boundary_condition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = NeuralWavefunction(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis Sampling\n",
    "\n",
    "To sample the wave function, we'll use the metropolis algorithm.  We can implement this algorithm in tensorflow as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MetropolisSampler(object):\n",
    "    \"\"\"Metropolis Sampler in N dimension\n",
    "\n",
    "    Sample from N-D coordinates, using some initial probability distribution\n",
    "\n",
    "    Relies on functional calls to sample on the fly with flexible distributions\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        n           : int,\n",
    "        nwalkers    : int,\n",
    "        initializer : callable,\n",
    "        init_params : iter ,\n",
    "        dtype       = tf.float64):\n",
    "        '''Initialize a metropolis sampler\n",
    "\n",
    "        Create a metropolis walker with `n` walkers.  Can use normal, uniform\n",
    "\n",
    "        Arguments:\n",
    "            n {int} -- Dimension\n",
    "            nwalkers {int} -- Number of unique walkers\n",
    "            initializer {callable} -- Function to call to initialize each walker\n",
    "            init_params {iter} -- Parameters to pass to the initializer, unrolled automatically\n",
    "        '''\n",
    "\n",
    "        # Set the dimension:\n",
    "        self.n = n\n",
    "\n",
    "        # Set the number of walkers:\n",
    "        self.nwalkers = nwalkers\n",
    "\n",
    "        self.size = (self.nwalkers, self.n)\n",
    "\n",
    "        self.dtype = dtype\n",
    "\n",
    "        #  Run the initalize to get the first locations:\n",
    "        self.walkers = initializer(shape=self.size, **init_params, dtype=dtype)\n",
    "\n",
    "    def sample(self):\n",
    "        '''Just return the current locations\n",
    "\n",
    "        '''\n",
    "        # Make sure to wrap in tf.Variable for back prop calculations\n",
    "        return  self.walkers\n",
    "\n",
    "    def kick(self,\n",
    "        wavefunction : tf.keras.models.Model,\n",
    "        kicker : callable,\n",
    "        kicker_params : iter,\n",
    "        nkicks : int ):\n",
    "        '''Wrapper for a compiled kick function via tensorflow.\n",
    "\n",
    "        This fills in the compiled function with the size and the walkers.\n",
    "\n",
    "        Arguments:\n",
    "            wavefunction {tf.keras.models.Model} -- The wavefunction used for the metropolis walk\n",
    "            kicker {callable} -- A callable function for generating kicks\n",
    "            kicker_params {iter} -- Arguments to the kicker function.\n",
    "        '''\n",
    "        # for i in range(nkicks):\n",
    "        walkers, acceptance = self.internal_kicker(\n",
    "            self.size, self.walkers, wavefunction, kicker, kicker_params, tf.constant(nkicks), dtype=self.dtype)\n",
    "\n",
    "        # Update the walkers:\n",
    "        self.walkers = walkers\n",
    "\n",
    "        # Send back the acceptance:\n",
    "        return acceptance\n",
    "\n",
    "    @tf.function\n",
    "    def internal_kicker(self,\n",
    "        shape,\n",
    "        walkers,\n",
    "        wavefunction : tf.keras.models.Model,\n",
    "        kicker : callable,\n",
    "        kicker_params : iter,\n",
    "        nkicks : tf.constant,\n",
    "        dtype):\n",
    "        \"\"\"Sample points in N-d Space\n",
    "\n",
    "        By default, samples points uniformly across all dimensions.\n",
    "        Returns a torch tensor on the chosen device with gradients enabled.\n",
    "\n",
    "        Keyword Arguments:\n",
    "            kicker {callable} -- Function to call to create a kick for each walker\n",
    "            kicker_params {iter} -- Parameters to pass to the kicker, unrolled automatically\n",
    "        \"\"\"\n",
    "\n",
    "        # Drop the model to reduced precision for this:\n",
    "        # params = wavefunction.parameters()\n",
    "        # print(params)\n",
    "\n",
    "        # reduced_wf = tf.cast(wavefunction, dtype=self.dtype)\n",
    "        # wavefunction.cast(self.dtype)\n",
    "\n",
    "\n",
    "        # We need to compute the wave function twice:\n",
    "        # Once for the original coordiate, and again for the kicked coordinates\n",
    "        acceptance = tf.convert_to_tensor(0.0, dtype=dtype)\n",
    "        # Calculate the current wavefunction value:\n",
    "        current_wavefunction = wavefunction(walkers)\n",
    "\n",
    "        # Generate a long set of random number from which we will pull:\n",
    "        random_numbers = tf.math.log(tf.random.uniform(shape = [nkicks,shape[0],1], dtype=dtype))\n",
    "\n",
    "        # Generate a long list of kicks:\n",
    "        # print(shape)\n",
    "        kicks = kicker(shape=[nkicks, *shape], **kicker_params, dtype=dtype)\n",
    "        # print(kicks.shape)\n",
    "\n",
    "        for i_kick in tf.range(nkicks):\n",
    "            # Create a kick:\n",
    "            kick = kicks[i_kick]\n",
    "            # kick = kicker(shape=shape, **kicker_params, dtype=dtype)\n",
    "            kicked = walkers + kick\n",
    "\n",
    "            # Compute the values of the wave function, which should be of shape\n",
    "            # [nwalkers, 1]\n",
    "            kicked_wavefunction   = wavefunction(kicked)\n",
    "\n",
    "\n",
    "            # Probability is the ratio of kicked **2 to original\n",
    "            probability = 2 * (kicked_wavefunction - current_wavefunction)\n",
    "            # Acceptance is whether the probability for that walker is greater than\n",
    "            # a random number between [0, 1).\n",
    "            # Pull the random numbers and create a boolean array\n",
    "            # accept      = probability >  tf.random.uniform(shape=[shape[0],1])\n",
    "            accept      = probability >  random_numbers[i_kick]\n",
    "            # accept      = probability >  tf.math.log(tf.random.uniform(shape=[shape[0],1]))\n",
    "\n",
    "            # Grab the kicked wavefunction in the places it is new, to speed up metropolis:\n",
    "            current_wavefunction = tf.where(accept, kicked_wavefunction, current_wavefunction)\n",
    "\n",
    "            # We need to broadcast accept to match the right shape\n",
    "            # Needs to come out to the shape [nwalkers, nparticles, ndim]\n",
    "            accept = tf.tile(accept, [1,tf.reduce_prod(shape[1:])])\n",
    "            accept = tf.reshape(accept, shape)\n",
    "            walkers = tf.where(accept, kicked, walkers)\n",
    "\n",
    "            acceptance = tf.reduce_mean(tf.cast(accept, dtype))\n",
    "\n",
    "        return walkers, acceptance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a surrogate model and a sampling tool, we can test things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = MetropolisSampler(\n",
    "            n           = 3, # 3 dimensions\n",
    "            nwalkers    = N_WALKERS,\n",
    "            initializer = tf.random.normal,\n",
    "            init_params = {\"mean\": 0.0, \"stddev\" : 0.2},\n",
    "            dtype       = DEFAULT_TENSOR_TYPE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sampler.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at X, we should see a 100 x 3 tensor.  Elements from the tensor should roughly follow a gaussian distribution since that's what we used to initialize it (mean 0, STD 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print a summary of the network, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"neural_wavefunction\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  192       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  64        \n",
      "=================================================================\n",
      "Total params: 4,352\n",
      "Trainable params: 4,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "values = wf(x)\n",
    "wf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermalize the walkers\n",
    "\n",
    "To thermalize the walkers, we can \"kick\" them a number of times according to the metropolis algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptance = sampler.kick(\n",
    "    wavefunction  = wf ,\n",
    "    kicker        = tf.random.normal,\n",
    "    kicker_params = {\"mean\": 0.0, \"stddev\" : 0.2},\n",
    "    nkicks        = 5000 )\n",
    "# This gets the latest walkers\n",
    "x = sampler.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.915, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(acceptance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrogen Atom Hamiltonian\n",
    "\n",
    "Here's an implementation of the hydrogen atom that computes observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class HydrogenAtom(object):\n",
    "    \"\"\"Harmonic Oscillator Potential\n",
    "\n",
    "    Implementation of the quantum harmonic oscillator hamiltonian\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        ''' Initialize the Hamiltonian\n",
    "\n",
    "        The derived class will check parameters, but this converts all of them to floats\n",
    "        and scores as TF Constants.\n",
    "\n",
    "        '''\n",
    "        object.__init__(self)\n",
    "        self.parameters = {\"mass\" : 1.0}\n",
    "        # Cast them all to tf constants:\n",
    "        for key in self.parameters:\n",
    "            self.parameters[key] = tf.constant(float(self.parameters[key]),dtype=DEFAULT_TENSOR_TYPE)\n",
    "\n",
    "        self.HBAR = tf.constant(1.0, dtype = DEFAULT_TENSOR_TYPE)\n",
    "        self.ELECTRON_CHARGE = tf.constant(1.0, dtype = DEFAULT_TENSOR_TYPE)\n",
    "\n",
    "    @tf.function\n",
    "    def potential_energy(self, *, inputs, Z):\n",
    "        \"\"\"Return potential energy\n",
    "\n",
    "        If the potential energy is already computed, and no arguments are supplied,\n",
    "        return the cached value\n",
    "\n",
    "        If all arguments are supplied, calculate and return the PE.\n",
    "\n",
    "        Otherwise, exception\n",
    "\n",
    "        Arguments:\n",
    "            inputs {tf.tensor} -- Tensor of shape [N, nparticles, dimension]\n",
    "            Z {tf.tensor} -- Atomic number\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor - potential energy of shape [1]\n",
    "        \"\"\"\n",
    "\n",
    "        # Potential energy is, for n particles, two pieces:\n",
    "        # Compute r_i, where r_i = sqrt(sum(x_i^2, y_i^2, z_i^2)) (in 3D)\n",
    "        # PE_1 = -(Z e^2)/(4 pi eps_0) * sum_i (1/r_i)\n",
    "        # Second, compute r_ij, for all i != j, and then\n",
    "        # PE_2 = -(e^2) / (4 pi eps_0) * sum_{i!=j} (1 / r_ij)\n",
    "        # where r_ij = sqrt( [xi - xj]^2 + [yi - yj] ^2 + [zi - zj]^2)\n",
    "\n",
    "        # Compute r\n",
    "        # Square the coordinates and sum for each walker\n",
    "        r = tf.math.sqrt(tf.reduce_sum(inputs**2, axis=1))\n",
    "        # This is the sum of 1/r for all particles with the nucleus:\n",
    "        pe_1 = - (Z * self.ELECTRON_CHARGE**2 ) *  1. / (r + 1e-8)\n",
    "\n",
    "        # This is the sum of 1/r for all particles with other particles.\n",
    "        # n_particles = inputs.shape[1]\n",
    "        # for i_particle in range(n_particles):\n",
    "        #     centroid = inputs[:,i_particle,:]\n",
    "        #\n",
    "        #     r = tf.math.sqrt(tf.reduce_sum((inputs -centroid)**2, axis=2))\n",
    "        #     pe_2 = -0.5* (ELECTRON_CHARGE**2 ) * tf.reduce_sum( 1. / (r + 1e-8), axis=1 )\n",
    "        #     # Because this force is symmetric, I'm multiplying by 0.5 to prevent overflow\n",
    "        pe_2 = 0.\n",
    "        return pe_1 + pe_2\n",
    "    \n",
    "    @tf.function\n",
    "    def kinetic_energy_jf(self, *, dlogw_dx, M):\n",
    "        \"\"\"Return Kinetic energy\n",
    "\n",
    "        Calculate and return the KE directly\n",
    "\n",
    "        Otherwise, exception\n",
    "\n",
    "        Arguments:\n",
    "            dlogw_of_x/dx {tf.Tensor} -- Computed derivative of the wavefunction\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor - kinetic energy (JF) of shape [1]\n",
    "        \"\"\"\n",
    "        # < x | KE | psi > / < x | psi > =  1 / 2m [ < x | p | psi > / < x | psi >  = 1/2 w * x**2\n",
    "\n",
    "        # Contract d2_w_dx over spatial dimensions and particles:\n",
    "        ke_jf = (self.HBAR**2 / (2 * M)) * tf.reduce_sum(dlogw_dx**2, axis=(1))\n",
    "\n",
    "        return ke_jf\n",
    "\n",
    "    @tf.function\n",
    "    def kinetic_energy(self, *, KE_JF : tf.Tensor, d2logw_dx2 : tf.Tensor, M):\n",
    "        \"\"\"Return Kinetic energy\n",
    "\n",
    "\n",
    "        If all arguments are supplied, calculate and return the KE.\n",
    "\n",
    "        Arguments:\n",
    "            d2logw_dx2 {tf.Tensor} -- Computed second derivative of the wavefunction\n",
    "            KE_JF {tf.Tensor} -- JF computation of the kinetic energy\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor - potential energy of shape [1]\n",
    "        \"\"\"\n",
    "\n",
    "        ke = -(self.HBAR**2 / (2 * M)) * tf.reduce_sum(d2logw_dx2, axis=(1))\n",
    "        ke = ke  - KE_JF\n",
    "\n",
    "        return ke\n",
    "\n",
    "    @tf.function\n",
    "    def compute_derivatives(self, wavefunction : tf.keras.models.Model, inputs : tf.Tensor):\n",
    "\n",
    "\n",
    "        # Turning off all tape watching except for the inputs:\n",
    "        # Using the outer-most tape to watch the computation of the first derivative:\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Use the inner tape to watch the computation of the wavefunction:\n",
    "            tape.watch(inputs)\n",
    "            with tf.GradientTape() as second_tape:\n",
    "                second_tape.watch(inputs)\n",
    "                logw_of_x = wavefunction(inputs, training=True)\n",
    "            # Get the derivative of logw_of_x with respect to inputs\n",
    "            dlogw_dx = second_tape.gradient(logw_of_x, inputs)\n",
    "\n",
    "        # Get the derivative of dlogw_dx with respect to inputs (aka second derivative)\n",
    "\n",
    "        # We have to extract the diagonal of the jacobian, which comes out with shape\n",
    "        # [nwalkers, nparticles, dimension, nwalkers, nparticles, dimension]\n",
    "\n",
    "        # This is the full hessian computation:\n",
    "        d2logw_dx2 = tape.batch_jacobian(dlogw_dx, inputs)\n",
    "\n",
    "        \n",
    "        # And this contracts:\n",
    "        d2logw_dx2 = tf.einsum(\"wdd->wd\",d2logw_dx2)\n",
    "\n",
    "        return logw_of_x, dlogw_dx, d2logw_dx2\n",
    "\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def compute_energies(self, inputs, logw_of_x, dlogw_dx, d2logw_dx2):\n",
    "        '''Compute PE, KE_JF, and KE_direct\n",
    "\n",
    "        Harmonic Oscillator Energy Calculations\n",
    "\n",
    "        Arguments:\n",
    "            inputs {[type]} -- walker coordinates (shape is [nwalkers, nparticles, dimension])\n",
    "            logw_of_x {[type]} -- computed wave function at each walker\n",
    "            dlogw_dx {[type]} -- first derivative of wavefunction at each walker\n",
    "            d2logw_dx2 {[type]} -- second derivative of wavefunction at each walker\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError -- [description]\n",
    "\n",
    "        Returns:\n",
    "            pe -- potential energy\n",
    "            ke_jf -- JF Kinetic energy\n",
    "            ke_direct -- 2nd deriv computation of potential energy\n",
    "        '''\n",
    "\n",
    "        # Potential energy depends only on the wavefunction\n",
    "        pe = self.potential_energy(inputs=inputs, Z=1)\n",
    "\n",
    "        # KE by parts needs only one derivative\n",
    "        ke_jf = self.kinetic_energy_jf(dlogw_dx=dlogw_dx, M=self.parameters[\"mass\"])\n",
    "\n",
    "        # True, directly, uses the second derivative\n",
    "        ke_direct = self.kinetic_energy(KE_JF = ke_jf, d2logw_dx2 = d2logw_dx2, M=self.parameters[\"mass\"])\n",
    "\n",
    "\n",
    "        return pe, ke_jf, ke_direct\n",
    "\n",
    "    @tf.function\n",
    "    def energy(self, wavefunction : tf.keras.models.Model, inputs : tf.Tensor):\n",
    "        \"\"\"Compute the expectation value of energy of the supplied wavefunction.\n",
    "\n",
    "        Computes the integral of the wavefunction in this potential\n",
    "\n",
    "        Arguments:\n",
    "            wavefunction {Wavefunction model} -- Callable wavefunction object\n",
    "            inputs {tf.Tensor} -- Tensor of shape [nwalkers, nparticles, dimension]\n",
    "\n",
    "        Returns:\n",
    "            tf.tensor - energy of shape [n_walkers]\n",
    "            tf.tensor - energy_jf of shape [n_walkers]\n",
    "            tf.tensor - ke_jf of shape [n_walkers]\n",
    "            tf.tensor - ke_direct of shape [n_walkers]\n",
    "            tf.tensor - pe of shape [n_walkers]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # This function takes the inputs\n",
    "        # And computes the expectation value of the energy at each input point\n",
    "\n",
    "        logw_of_x, dlogw_dx, d2logw_dx2 = self.compute_derivatives(wavefunction, inputs)\n",
    "\n",
    "        pe, ke_jf, ke_direct = self.compute_energies(inputs, logw_of_x, dlogw_dx, d2logw_dx2)\n",
    "\n",
    "        # Total energy computations:\n",
    "        energy = tf.squeeze(pe+ke_direct)\n",
    "        energy_jf = tf.squeeze(pe+ke_jf)\n",
    "\n",
    "        return energy, energy_jf, ke_jf, ke_direct, pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamiltonian = HydrogenAtom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "energy, energy_jf, ke_jf, ke_direct, pe = hamiltonian.energy(wf, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-0.3614002, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(energy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the wavefunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the wavefunction, we'll need to compute several observables based on the jacobian.  Here are some functions to help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def jacobian(x_current, wavefunction):\n",
    "    tape = tf.GradientTape()\n",
    "\n",
    "    with tape:\n",
    "        tape.watch(wavefunction.trainable_variables)\n",
    "        log_wpsi = wavefunction(x_current)\n",
    "\n",
    "#     print(log_wpsi)\n",
    "        \n",
    "    \n",
    "    jac = tape.jacobian(log_wpsi, wavefunction.trainable_variables)\n",
    "    \n",
    "#     print(wavefunction.trainable_variables)\n",
    "#     print(jac)\n",
    "    \n",
    "    # Grab the original shapes ([1:] means everything except first dim):\n",
    "    jac_shape = [j.shape[1:] for j in jac]\n",
    "    # get the flattened shapes:\n",
    "    flat_shape = [[-1, tf.reduce_prod(js)] for js in jac_shape]\n",
    "    # Reshape the\n",
    "\n",
    "    # We have the flat shapes and now we need to make the jacobian into a single matrix\n",
    "\n",
    "    flattened_jacobian = [tf.reshape(j, f) for j, f in zip(jac, flat_shape)]\n",
    "\n",
    "    flattened_jacobian = tf.concat(flattened_jacobian, axis=-1)\n",
    "\n",
    "    return flattened_jacobian, flat_shape\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def compute_O_observables(flattened_jacobian, energy):\n",
    "\n",
    "    # dspi_i is the reduction of the jacobian over all walkers.\n",
    "    # In other words, it's the mean gradient of the parameters with respect to inputs.\n",
    "    # This is effectively the measurement of O^i in the paper.\n",
    "    dpsi_i = tf.reduce_mean(flattened_jacobian, axis=0)\n",
    "    dpsi_i = tf.reshape(dpsi_i, [-1,1])\n",
    "\n",
    "    # Computing <O^m H>:\n",
    "    dpsi_i_EL = tf.linalg.matmul(tf.reshape(energy, [1,N_WALKERS]), flattened_jacobian)\n",
    "    # This makes this the same shape as the other tensors\n",
    "    dpsi_i_EL = tf.reshape(dpsi_i_EL, [-1, 1])\n",
    "\n",
    "    return dpsi_i, dpsi_i_EL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, energy: -0.3186022639274597\n",
      "Iteration 1, energy: -0.2526441812515259\n",
      "Iteration 2, energy: -0.24255509674549103\n",
      "Iteration 3, energy: -0.23867547512054443\n",
      "Iteration 4, energy: -0.2578572630882263\n",
      "Iteration 5, energy: -0.28064462542533875\n",
      "Iteration 6, energy: -0.34151560068130493\n",
      "Iteration 7, energy: -0.3379077613353729\n",
      "Iteration 8, energy: -0.3110887408256531\n",
      "Iteration 9, energy: -0.3043680191040039\n",
      "Iteration 10, energy: -0.32603803277015686\n",
      "Iteration 11, energy: -0.35143977403640747\n",
      "Iteration 12, energy: -0.35750070214271545\n",
      "Iteration 13, energy: -0.33347398042678833\n",
      "Iteration 14, energy: -0.33731746673583984\n",
      "Iteration 15, energy: -0.34573617577552795\n",
      "Iteration 16, energy: -0.3476046919822693\n",
      "Iteration 17, energy: -0.3546047508716583\n",
      "Iteration 18, energy: -0.35573381185531616\n",
      "Iteration 19, energy: -0.3493567705154419\n",
      "Iteration 20, energy: -0.3545118272304535\n",
      "Iteration 21, energy: -0.35066962242126465\n",
      "Iteration 22, energy: -0.3504236340522766\n",
      "Iteration 23, energy: -0.35606926679611206\n",
      "Iteration 24, energy: -0.3475015461444855\n",
      "Iteration 25, energy: -0.3526478409767151\n",
      "Iteration 26, energy: -0.3528582453727722\n",
      "Iteration 27, energy: -0.35241153836250305\n",
      "Iteration 28, energy: -0.34850627183914185\n",
      "Iteration 29, energy: -0.35777798295021057\n",
      "Iteration 30, energy: -0.3513241410255432\n",
      "Iteration 31, energy: -0.34660980105400085\n",
      "Iteration 32, energy: -0.3508692979812622\n",
      "Iteration 33, energy: -0.35194411873817444\n",
      "Iteration 34, energy: -0.35564953088760376\n",
      "Iteration 35, energy: -0.3550489842891693\n",
      "Iteration 36, energy: -0.35445547103881836\n",
      "Iteration 37, energy: -0.34475502371788025\n",
      "Iteration 38, energy: -0.3446577489376068\n",
      "Iteration 39, energy: -0.3563615679740906\n",
      "Iteration 40, energy: -0.3589214086532593\n",
      "Iteration 41, energy: -0.34935200214385986\n",
      "Iteration 42, energy: -0.3502841293811798\n",
      "Iteration 43, energy: -0.3599620461463928\n",
      "Iteration 44, energy: -0.35037505626678467\n",
      "Iteration 45, energy: -0.35291004180908203\n",
      "Iteration 46, energy: -0.349671870470047\n",
      "Iteration 47, energy: -0.35219937562942505\n",
      "Iteration 48, energy: -0.3547605872154236\n",
      "Iteration 49, energy: -0.3472079038619995\n",
      "Iteration 50, energy: -0.35633501410484314\n",
      "Iteration 51, energy: -0.35082608461380005\n",
      "Iteration 52, energy: -0.34618180990219116\n",
      "Iteration 53, energy: -0.34783485531806946\n",
      "Iteration 54, energy: -0.3552404046058655\n",
      "Iteration 55, energy: -0.34407809376716614\n",
      "Iteration 56, energy: -0.35893163084983826\n",
      "Iteration 57, energy: -0.3612444996833801\n",
      "Iteration 58, energy: -0.3533361554145813\n",
      "Iteration 59, energy: -0.3570396602153778\n",
      "Iteration 60, energy: -0.3513484597206116\n",
      "Iteration 61, energy: -0.3526041805744171\n",
      "Iteration 62, energy: -0.3539229929447174\n",
      "Iteration 63, energy: -0.35622239112854004\n",
      "Iteration 64, energy: -0.3555365204811096\n",
      "Iteration 65, energy: -0.358676016330719\n",
      "Iteration 66, energy: -0.3531724810600281\n",
      "Iteration 67, energy: -0.3470187783241272\n",
      "Iteration 68, energy: -0.35212939977645874\n",
      "Iteration 69, energy: -0.35444238781929016\n",
      "Iteration 70, energy: -0.3586841821670532\n",
      "Iteration 71, energy: -0.3444635272026062\n",
      "Iteration 72, energy: -0.35961660742759705\n",
      "Iteration 73, energy: -0.3545425832271576\n",
      "Iteration 74, energy: -0.34540486335754395\n",
      "Iteration 75, energy: -0.3455958962440491\n",
      "Iteration 76, energy: -0.3529881536960602\n",
      "Iteration 77, energy: -0.35390806198120117\n",
      "Iteration 78, energy: -0.35358574986457825\n",
      "Iteration 79, energy: -0.3591874837875366\n",
      "Iteration 80, energy: -0.35065966844558716\n",
      "Iteration 81, energy: -0.350107342004776\n",
      "Iteration 82, energy: -0.3489258885383606\n",
      "Iteration 83, energy: -0.3559315800666809\n",
      "Iteration 84, energy: -0.35365813970565796\n",
      "Iteration 85, energy: -0.3538319766521454\n",
      "Iteration 86, energy: -0.35913145542144775\n",
      "Iteration 87, energy: -0.35320284962654114\n",
      "Iteration 88, energy: -0.3483181893825531\n",
      "Iteration 89, energy: -0.3476276695728302\n",
      "Iteration 90, energy: -0.3700418174266815\n",
      "Iteration 91, energy: -0.35401684045791626\n",
      "Iteration 92, energy: -0.351464182138443\n",
      "Iteration 93, energy: -0.35217171907424927\n",
      "Iteration 94, energy: -0.35527557134628296\n",
      "Iteration 95, energy: -0.3672661781311035\n",
      "Iteration 96, energy: -0.3560790419578552\n",
      "Iteration 97, energy: -0.35211679339408875\n",
      "Iteration 98, energy: -0.35145625472068787\n",
      "Iteration 99, energy: -0.35241734981536865\n",
      "Iteration 100, energy: -0.3487204611301422\n",
      "Iteration 101, energy: -0.3555215299129486\n",
      "Iteration 102, energy: -0.3526293635368347\n",
      "Iteration 103, energy: -0.3522992432117462\n",
      "Iteration 104, energy: -0.3631601333618164\n",
      "Iteration 105, energy: -0.35930752754211426\n",
      "Iteration 106, energy: -0.3474138081073761\n",
      "Iteration 107, energy: -0.35717082023620605\n",
      "Iteration 108, energy: -0.35509514808654785\n",
      "Iteration 109, energy: -0.35408905148506165\n",
      "Iteration 110, energy: -0.3504103422164917\n",
      "Iteration 111, energy: -0.34995725750923157\n"
     ]
    }
   ],
   "source": [
    "# How many times should we make an observation of the observables each iteration?\n",
    "N_OBSERVATIONS = 20\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "for i in range(500):\n",
    "    \n",
    "    dpsi_i = None\n",
    "    dpsi_i_EL = None\n",
    "    obs_energy = 0\n",
    "    \n",
    "    for i_obs in range(N_OBSERVATIONS):\n",
    "    \n",
    "        # First, we kick the sampler to re-thermalize to the new wavefunction:\n",
    "        kicker = tf.random.normal\n",
    "        kicker_params = {\"mean\": 0.0, \"stddev\" : 0.4}\n",
    "\n",
    "        acceptance = sampler.kick(wf, kicker, kicker_params, nkicks=100)\n",
    "\n",
    "\n",
    "        # Get the current walker locations:\n",
    "        x_current  = sampler.sample()\n",
    "\n",
    "        # Compute the observables:\n",
    "        energy, energy_jf, ke_jf, ke_direct, pe = hamiltonian.energy(wf, x_current)\n",
    "\n",
    "        energy /= N_WALKERS\n",
    "\n",
    "\n",
    "        # For each observation, we compute the jacobian.\n",
    "        # flattened_jacobian is a list, flat_shape is just one instance\n",
    "        flattened_jacobian, flat_shape = jacobian(x_current, wf)\n",
    "\n",
    "        _dpsi_i, _dpsi_i_EL = compute_O_observables(flattened_jacobian, energy)\n",
    "        if dpsi_i is None:\n",
    "            dpsi_i = _dpsi_i\n",
    "        else:\n",
    "            dpsi_i += _dpsi_i\n",
    "            \n",
    "        if dpsi_i_EL is None:\n",
    "            dpsi_i_EL = _dpsi_i_EL\n",
    "        else:\n",
    "            dpsi_i_EL += _dpsi_i_EL\n",
    "\n",
    "        obs_energy += tf.reduce_sum(energy)\n",
    "        \n",
    "    obs_energy /= N_OBSERVATIONS\n",
    "    dpsi_i     /= N_OBSERVATIONS\n",
    "    dpsi_i_EL  /= N_OBSERVATIONS\n",
    "    \n",
    "    gradients = - 2*( dpsi_i * obs_energy - dpsi_i_EL )\n",
    "\n",
    "    # Lastly, reshape the gradients to match the weights:\n",
    "    running_index = 0\n",
    "    gradient = []\n",
    "    for length in flat_shape:\n",
    "        l = length[-1].numpy()\n",
    "        end_index = running_index + l\n",
    "        gradient.append(gradients[running_index:end_index])\n",
    "        running_index += l\n",
    "    \n",
    "    shapes = [ p.shape for p in wf.trainable_variables ]\n",
    "    gradients = [ tf.reshape(g, s) for g, s in zip(gradient, shapes)]\n",
    "    \n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, wf.trainable_variables))\n",
    "    \n",
    "    print(f\"Iteration {i}, energy: {obs_energy}\")\n",
    "    \n",
    "#     print(gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
